import os
import torch
import torchvision
from pathlib import Path
from torch.utils.data import DataLoader
from torchvision.transforms import functional as F
import torchvision.transforms as transforms
from models.face_model import get_face_detector
from scripts.face_dataset import FaceDetectionDataset
from torch.cuda.amp import autocast, GradScaler
scaler = GradScaler()


from tqdm import tqdm

def collate_fn(batch):
    return tuple(zip(*batch))

def main():
    torch.backends.cudnn.benchmark = True
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ä½¿ç”¨è®¾å¤‡ï¼š{device}")

    # âœ… æ ¹ç›®å½•
    BASE_DIR = Path(__file__).resolve().parent
    CHECKPOINT_PATH = BASE_DIR / "checkpoint.pt"

    # âœ… æ•°æ®è·¯å¾„
    img_dir = BASE_DIR / "datasets" / "raw" / "images"
    ann_path = BASE_DIR / "datasets" / "annotations.json"

    # âœ… æ•°æ®å¢å¼º
    '''transform = transforms.Compose([
        transforms.Resize((256, 256)),
        transforms.ToTensor(),
    ])'''

    transform = transforms.ToTensor()


    MAX_SAMPLES = None  # æƒ³æµ‹è¯•å¤šå°‘å¼ å›¾åƒå°±å†™å¤šå°‘ï¼ŒNone è¡¨ç¤ºå…¨éƒ¨ä½¿ç”¨

    dataset = FaceDetectionDataset(str(img_dir), str(ann_path), transform=transform)

    if MAX_SAMPLES is not None:
        from torch.utils.data import Subset
        dataset = Subset(dataset, list(range(min(MAX_SAMPLES, len(dataset)))))
    
    dataloader = DataLoader(
        dataset,
        batch_size=1,
        shuffle=True,
        num_workers=8,
        pin_memory=True,
        collate_fn=collate_fn
    )

    # âœ… åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨
    model = get_face_detector(num_classes=2).to(device)
    optimizer = torch.optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.0001)
    start_epoch = 0

    # âœ… å¦‚æœæœ‰ checkpoint å°±æ¢å¤
    if CHECKPOINT_PATH.exists():
        checkpoint = torch.load(CHECKPOINT_PATH, map_location=device)
        model.load_state_dict(checkpoint['model'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        start_epoch = checkpoint['epoch']
        print(f"ğŸ”„ æ¢å¤æ¨¡å‹ï¼Œä»ç¬¬ {start_epoch} è½®ç»§ç»­è®­ç»ƒ")
    else:
        print("ğŸš€ ä»å¤´å¼€å§‹è®­ç»ƒ")

    # âœ… å¼€å§‹è®­ç»ƒ
    epochs = 5
    for epoch in range(start_epoch, epochs):
        model.train()
        total_loss = 0.0

        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")

        for images, targets in progress_bar:
            try:
                images = [img.to(device) for img in images]
                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

                with autocast():  # âœ… å¼€å¯æ··åˆç²¾åº¦
                    loss_dict = model(images, targets)
                    losses = sum(loss for loss in loss_dict.values())

                optimizer.zero_grad()
                scaler.scale(losses).backward()
                scaler.step(optimizer)
                scaler.update()

                total_loss += losses.item()
                progress_bar.set_postfix(loss=losses.item())

            except Exception as e:
                print(f"âš ï¸ è·³è¿‡å¼‚å¸¸æ‰¹æ¬¡: {e}")
                continue

        print(f"ğŸ“˜ Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}")

        # âœ… ä¿å­˜ checkpoint
        torch.save({
            'model': model.state_dict(),
            'optimizer': optimizer.state_dict(),
            'epoch': epoch + 1
        }, CHECKPOINT_PATH)

    print("âœ… æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶ä¿å­˜")

if __name__ == "__main__":
    import multiprocessing
    multiprocessing.freeze_support()
    main()
